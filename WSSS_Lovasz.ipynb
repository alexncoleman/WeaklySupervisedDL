{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QHUC0F9AOQqM","outputId":"e222d10f-1833-4a9b-c152-92cfcb842281","executionInfo":{"status":"ok","timestamp":1744571665788,"user_tz":-60,"elapsed":33224,"user":{"displayName":"Ajay Mahendrakumaran","userId":"10969685641892920517"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]},{"output_type":"stream","name":"stderr","text":["/content/lovasz_losses.py:186: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n","  if (classes is 'present' and fg.sum() == 0):\n"]}],"source":["# --- AffinityNet Full Stage 1 + Stage 2 Implementation (with Random Walk) ---\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torchvision import transforms, models\n","from torchvision.models import resnet50\n","from torchvision.datasets import OxfordIIITPet\n","from torch.utils.data import DataLoader\n","import matplotlib.pyplot as plt\n","import random\n","import gc\n","from google.colab import drive\n","drive.mount('/content/drive')\n","import os\n","from lovasz_losses import lovasz_softmax  # Import Lovász-Softmax loss\n","\n","def download_data(pth=None, split=\"trainval\"):\n","    image_transform = transforms.Compose([\n","        transforms.Resize((224, 224), interpolation=transforms.InterpolationMode.BICUBIC),\n","        transforms.ToTensor()\n","    ])\n","    mask_transform = transforms.Compose([\n","        transforms.Resize((224, 224), interpolation=transforms.InterpolationMode.BICUBIC),\n","        transforms.PILToTensor()\n","    ])\n","    dataset = OxfordIIITPet(\n","        root=pth,\n","        split=split,\n","        target_types=(\"category\", \"segmentation\"),\n","        download=True,\n","        transform=image_transform,\n","        target_transform=lambda t: (t[0], mask_transform(t[1]))\n","    )\n","    return dataset\n","\n","# --- Classification Model ---\n","class FrozenResNetCAM(nn.Module):\n","    def __init__(self, num_classes=37):\n","        super().__init__()\n","        resnet = resnet50(pretrained=True, replace_stride_with_dilation=[False, False, True])\n","        for param in resnet.parameters():\n","            param.requires_grad = False\n","\n","        # Split layers for easier access\n","\n","        self.layer0 = nn.Sequential(resnet.conv1, resnet.bn1, resnet.relu, resnet.maxpool)\n","        self.layer1 = resnet.layer1\n","        self.layer2 = resnet.layer2\n","        self.layer3 = resnet.layer3\n","        self.layer4 = resnet.layer4\n","        # self.layer5 = nn.Sequential(resnet.conv1, resnet.bn1, resnet.relu)\n","\n","        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n","        self.fc = nn.Linear(resnet.fc.in_features, num_classes)\n","\n","    def forward(self, x):\n","        x = self.layer0(x)\n","        f1 = self.layer1(x)\n","        f2 = self.layer2(f1)\n","        f3 = self.layer3(f2)\n","        f4 = self.layer4(f3)\n","        # f5 = self.layer5(f4)\n","\n","        pooled = self.avgpool(f4)\n","        flat = pooled.view(pooled.size(0), -1)\n","        logits = self.fc(flat)\n","\n","        # Return all feature maps for LayerCAM\n","        return logits, [f2, f3, f4]\n","\n","class LayerCAMGenerator:\n","    def __init__(self, model, target_layer_names=[\"layer2\", \"layer3\", \"layer4\"]):\n","        self.model = model.eval()\n","        self.target_layer_names = target_layer_names\n","\n","        self.activations = {}\n","        self.gradients = {}\n","\n","        self._register_hooks()\n","\n","    def _register_hooks(self):\n","        for name in self.target_layer_names:\n","            layer = getattr(self.model, name)\n","            layer.register_forward_hook(self._make_forward_hook(name))\n","            layer.register_full_backward_hook(self._make_backward_hook(name))\n","\n","    def _make_forward_hook(self, name):\n","        def hook(module, input, output):\n","            self.activations[name] = output\n","        return hook\n","\n","    def _make_backward_hook(self, name):\n","        def hook(module, grad_input, grad_output):\n","            self.gradients[name] = grad_output[0]\n","        return hook\n","\n","\n","    def generate(self, images, class_idx=None, alpha=1.0):\n","        self.activations.clear()\n","        self.gradients.clear()\n","\n","        images = images.unsqueeze(0)\n","        images = images.requires_grad_()\n","\n","        with torch.enable_grad():\n","            logits, _ = self.model(images)\n","\n","            if class_idx is None:\n","                class_idx = torch.argmax(logits, dim=1)\n","\n","            class_scores = logits.gather(1, class_idx.view(-1, 1)).squeeze()\n","            class_scores.backward(torch.ones_like(class_scores), retain_graph=True)\n","\n","        layer_cams = []\n","\n","        for name in self.target_layer_names:\n","            act = self.activations[name]     # (B, C, H, W)\n","            grad = self.gradients[name]      # (B, C, H, W)\n","\n","            with torch.no_grad():\n","                weights = F.relu(grad * act)\n","                cam = weights.sum(dim=1)         # (B, H, W)\n","                cam = F.relu(cam)\n","\n","                # Normalize per image (detach and operate on a copy)\n","                for i in range(cam.shape[0]):\n","                    c = cam[i]\n","                    c = c.detach()\n","                    c -= c.min()\n","                    c /= (c.max() + 1e-8)\n","                    cam[i] = c\n","\n","                cam = F.interpolate(cam.unsqueeze(1), size=(224, 224), mode=\"bilinear\", align_corners=False)\n","                layer_cams.append(cam.squeeze(1).detach())  # detach CAM from graph\n","\n","            del act, grad, weights, cam  # free up memory\n","\n","        final_cam = sum(layer_cams) / len(layer_cams)\n","        final_cam = final_cam.detach()\n","        final_cam = (final_cam).clamp(min=0.0) ** alpha\n","\n","        del logits, class_scores, images, layer_cams\n","        torch.cuda.empty_cache()\n","        gc.collect()\n","\n","        return final_cam  # (B, H, W)\n","\n","\n","    def generate_bg_cam(self, image_tensor, valid_class_indices, alpha=1.0):\n","        \"\"\"\n","        Mimics CAMGenerator's bg+fg map output for integration with AffinityNet.\n","        \"\"\"\n","        #all_cams = self.generate_all_cams(image_tensor)\n","        all_cams = self.generate(image_tensor, valid_class_indices)\n","\n","        mask = torch.zeros_like(all_cams)\n","        valid_cams = all_cams\n","\n","        max_obj_cam, _ = valid_cams.max(dim=0)  # (H, W)\n","        m_bg = 1.0 - ((1.0 - max_obj_cam).clamp(min=0.0) ** alpha)\n","\n","        # Resize for consistency\n","        m_bg_resized = F.interpolate(\n","            m_bg.unsqueeze(0).unsqueeze(0), size=(224, 224), mode='bilinear', align_corners=False\n","        ).squeeze()\n","\n","        max_obj_cam_resized = F.interpolate(\n","            max_obj_cam.unsqueeze(0).unsqueeze(0), size=(224, 224), mode='bilinear', align_corners=False\n","        ).squeeze()\n","\n","        return m_bg_resized, max_obj_cam_resized\n","\n","def train_fc_only(model, dataloader, device, epochs=10):\n","    model.to(device)\n","    model.train()\n","\n","    optimizer = torch.optim.Adam(model.fc.parameters(), lr=1e-3)  # Only train FC layer\n","    criterion = nn.CrossEntropyLoss()\n","\n","    for epoch in range(epochs):\n","        total_loss, correct, total = 0.0, 0, 0\n","        for imgs, (labels, _) in dataloader:\n","            imgs, labels = imgs.to(device), labels.to(device)\n","            logits, _ = model(imgs)\n","            loss = criterion(logits, labels)\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","\n","            total_loss += loss.item() * imgs.size(0)\n","            preds = logits.argmax(dim=1)\n","            correct += (preds == labels).sum().item()\n","            total += imgs.size(0)\n","\n","        print(f\"Epoch {epoch+1}/{epochs} - Loss: {total_loss / total:.4f} - Acc: {100 * correct / total:.2f}%\")\n","\n","    model.eval()"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ygv7H3N0GfLJ","outputId":"6fbd2e99-5390-4a4b-d310-18fe8d5556f3","executionInfo":{"status":"ok","timestamp":1744571704420,"user_tz":-60,"elapsed":38603,"user":{"displayName":"Ajay Mahendrakumaran","userId":"10969685641892920517"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 792M/792M [00:23<00:00, 33.7MB/s]\n","100%|██████████| 19.2M/19.2M [00:01<00:00, 12.5MB/s]\n"]}],"source":["#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","dataset = download_data(\"/content/OxfordIIITPetDataset\", split=\"trainval\")\n","loader = DataLoader(dataset, batch_size=16, shuffle=True)"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DauDj3iDf7Gv","outputId":"194d4604-9758-4ace-a08b-cfb37a466743","executionInfo":{"status":"ok","timestamp":1744571705720,"user_tz":-60,"elapsed":1298,"user":{"displayName":"Ajay Mahendrakumaran","userId":"10969685641892920517"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n","100%|██████████| 97.8M/97.8M [00:00<00:00, 195MB/s]\n"]}],"source":["classifier = FrozenResNetCAM().to(device)"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZXz1QPYaGfLJ","outputId":"1457eeb3-eee7-404b-c75f-f284a097d153","executionInfo":{"status":"ok","timestamp":1744572002765,"user_tz":-60,"elapsed":297043,"user":{"displayName":"Ajay Mahendrakumaran","userId":"10969685641892920517"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/10 - Loss: 1.2742 - Acc: 68.83%\n","Epoch 2/10 - Loss: 0.4267 - Acc: 87.91%\n","Epoch 3/10 - Loss: 0.3260 - Acc: 90.65%\n","Epoch 4/10 - Loss: 0.2974 - Acc: 90.62%\n","Epoch 5/10 - Loss: 0.2375 - Acc: 92.74%\n","Epoch 6/10 - Loss: 0.2075 - Acc: 93.59%\n","Epoch 7/10 - Loss: 0.1947 - Acc: 92.83%\n","Epoch 8/10 - Loss: 0.1746 - Acc: 94.43%\n","Epoch 9/10 - Loss: 0.1472 - Acc: 95.49%\n","Epoch 10/10 - Loss: 0.1509 - Acc: 94.70%\n"," Classifier trained.\n"]}],"source":["train_fc_only(classifier, loader, device, epochs=10)\n","print(\" Classifier trained.\")"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"x2otK9YHUTBa","outputId":"7c5e0092-f756-455b-b3fa-b4bac72e5d73","executionInfo":{"status":"ok","timestamp":1744572010526,"user_tz":-60,"elapsed":7760,"user":{"displayName":"Ajay Mahendrakumaran","userId":"10969685641892920517"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Weights saved to /content/drive/MyDrive/Colab_Notebooks/classifier_weights_without_affinity.pth\n"]}],"source":["# save_path = '/content/drive/MyDrive/Applied_Deep_Learning/classifier_weights_without_affinity.pth'\n","# torch.save(classifier.state_dict(), save_path)\n","\n","# print(f\"Weights saved to {save_path}\")\n","\n","save_dir = '/content/drive/MyDrive/Colab_Notebooks'\n","os.makedirs(save_dir, exist_ok=True)  #  Create parent folder if it doesn't exist\n","\n","save_path = os.path.join(save_dir, 'classifier_weights_without_affinity.pth')\n","torch.save(classifier.state_dict(), save_path)\n","\n","print(f\"Weights saved to {save_path}\")"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5WoCZEVjwWzd","outputId":"38213001-3c8a-4440-8cdf-2c42f9e38dfe","executionInfo":{"status":"ok","timestamp":1744572010801,"user_tz":-60,"elapsed":273,"user":{"displayName":"Ajay Mahendrakumaran","userId":"10969685641892920517"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["FrozenResNetCAM(\n","  (layer0): Sequential(\n","    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n","    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (2): ReLU(inplace=True)\n","    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n","  )\n","  (layer1): Sequential(\n","    (0): Bottleneck(\n","      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (downsample): Sequential(\n","        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): Bottleneck(\n","      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","    (2): Bottleneck(\n","      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","  )\n","  (layer2): Sequential(\n","    (0): Bottleneck(\n","      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (downsample): Sequential(\n","        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): Bottleneck(\n","      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","    (2): Bottleneck(\n","      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","    (3): Bottleneck(\n","      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","  )\n","  (layer3): Sequential(\n","    (0): Bottleneck(\n","      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (downsample): Sequential(\n","        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): Bottleneck(\n","      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","    (2): Bottleneck(\n","      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","    (3): Bottleneck(\n","      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","    (4): Bottleneck(\n","      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","    (5): Bottleneck(\n","      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","  )\n","  (layer4): Sequential(\n","    (0): Bottleneck(\n","      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (downsample): Sequential(\n","        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): Bottleneck(\n","      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n","      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","    (2): Bottleneck(\n","      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n","      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","  )\n","  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n","  (fc): Linear(in_features=2048, out_features=37, bias=True)\n",")"]},"metadata":{},"execution_count":6}],"source":["classifier.load_state_dict(torch.load(save_path, map_location=torch.device('cuda')))\n","classifier.eval()"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"EElYuoOJGfLL","executionInfo":{"status":"ok","timestamp":1744572010809,"user_tz":-60,"elapsed":7,"user":{"displayName":"Ajay Mahendrakumaran","userId":"10969685641892920517"}}},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import torchvision.transforms.functional as TF\n","\n","def overlay_cam_on_image(image_tensor, cam_tensor, alpha=0.5, colormap='gray'):\n","    \"\"\"\n","    Overlays a CAM heatmap on an input image.\n","\n","    Args:\n","        image_tensor (Tensor): Tensor of shape (3, H, W)\n","        cam_tensor (Tensor): Tensor of shape (H, W) with values in [0, 1]\n","        alpha (float): Opacity of the heatmap\n","        colormap (str): Matplotlib colormap to use\n","\n","    Returns:\n","        np.ndarray: Overlay image as (H, W, 3) numpy array\n","    \"\"\"\n","    # Convert image to numpy (H, W, 3)\n","    image = TF.to_pil_image(image_tensor.cpu())\n","    image_np = np.array(image).astype(np.float32) / 255.0\n","\n","    # Normalize cam and convert to heatmap\n","    try:\n","        cam = cam_tensor.cpu().numpy()\n","    except:\n","        cam = cam_tensor.cpu().detach().numpy()\n","    cam = (cam - cam.min()) / (cam.max() + 1e-8)\n","    heatmap = plt.get_cmap(colormap)(cam)[:, :, :3]  # Drop alpha channel\n","\n","    # Blend heatmap and image\n","    overlay = (1 - alpha) * image_np + alpha * heatmap\n","    overlay = np.clip(overlay, 0, 1)\n","\n","    return overlay"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"zwADQZnM0iQy","executionInfo":{"status":"ok","timestamp":1744572010815,"user_tz":-60,"elapsed":5,"user":{"displayName":"Ajay Mahendrakumaran","userId":"10969685641892920517"}}},"outputs":[],"source":["loader = DataLoader(dataset, batch_size=32, shuffle=True)"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"U2m_jJD0Suwr","executionInfo":{"status":"ok","timestamp":1744572010820,"user_tz":-60,"elapsed":4,"user":{"displayName":"Ajay Mahendrakumaran","userId":"10969685641892920517"}}},"outputs":[],"source":["test_dataset = download_data(\"/content/OxfordIIITPetDataset\", split=\"test\")\n","test_loader = DataLoader(test_dataset, batch_size=32, shuffle=True)"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"EZRI-tCwOWuK","executionInfo":{"status":"ok","timestamp":1744572011252,"user_tz":-60,"elapsed":431,"user":{"displayName":"Ajay Mahendrakumaran","userId":"10969685641892920517"}}},"outputs":[],"source":["from skimage.measure import label as lb, regionprops\n","def keep_largest(mask):\n","    labeled = lb(mask)\n","    regions = regionprops(labeled)\n","    if not regions:\n","        return mask\n","    largest = max(regions, key=lambda r: r.area)\n","    return (labeled == largest.label).astype(np.uint8)"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"5kY8RkR77dG4","executionInfo":{"status":"ok","timestamp":1744572011254,"user_tz":-60,"elapsed":1,"user":{"displayName":"Ajay Mahendrakumaran","userId":"10969685641892920517"}}},"outputs":[],"source":["def compute_iou_and_acc(pred_mask, true_mask):\n","    \"\"\"\n","    Computes binary IoU and pixel accuracy.\n","    Args:\n","        pred_mask: Tensor of shape (H, W), foreground = >0\n","        true_mask: Tensor of shape (H, W), foreground = >0\n","    \"\"\"\n","    pred_fg = (pred_mask > 0)\n","    true_fg = (true_mask > 0)\n","\n","    intersection = (pred_fg & true_fg).sum().item()\n","    union = (pred_fg | true_fg).sum().item()\n","    correct = (pred_mask == true_mask).sum().item()\n","    total = true_mask.numel()\n","\n","    iou = intersection / (union + 1e-8)\n","    acc = correct / total\n","    return iou, acc\n"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1ZG2n3naw7CLEe29kB_pHdic-GnJSLPXu"},"id":"dL8WpMCaSnf_","outputId":"8cbf4bfa-9a11-41cf-b695-18cf3d54e95a","executionInfo":{"status":"ok","timestamp":1744572043306,"user_tz":-60,"elapsed":32052,"user":{"displayName":"Ajay Mahendrakumaran","userId":"10969685641892920517"}}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["from os import PRIO_PROCESS\n","p = 0\n","layercam_gen = LayerCAMGenerator(classifier, target_layer_names=[\"layer3\", \"layer4\"])  # f2, f3, f4\n","device = \"cuda\"\n","import cv2\n","\n","for img, (label, true_mask) in test_loader:\n","    img = img[0].to(device)  # (3, H, W)\n","    true_mask = true_mask[0].to(device)\n","    for i in range(len(true_mask)):\n","        for j in range(len(true_mask[i])):\n","          for k in range(len(true_mask[i][j])):\n","            if true_mask[i][j][k] == 2 or true_mask[i][j][k] == 0:\n","              true_mask[i][j][k] = 0\n","            else:\n","              true_mask[i][j][k] = 1\n","\n","    label = label[0].item() if isinstance(label[0], torch.Tensor) else label[0]\n","\n","    valid_class_indices = [label]\n","\n","    ### --- LAYERCAM GENERATOR ---\n","    class_tensor = torch.tensor(valid_class_indices).to(device)\n","    layercam_output = layercam_gen.generate(img, class_idx=class_tensor, alpha=1.0)  # (1, H, W)\n","    layercam_bg, _ = layercam_gen.generate_bg_cam(img, class_tensor, alpha=1.9)\n","\n","    layercam = layercam_output.squeeze(0)  # (H, W)\n","    layercam[layercam < 0.3] = 0.0\n","\n","    layercam_bg[layercam_bg < 0.5] = 0.0\n","\n","    pred_bg_mask = torch.zeros_like(layercam_bg).long()\n","    pred_bg_mask[layercam_bg > 0.0] = 1\n","\n","    pred_fg_mask = torch.zeros_like(layercam).long()\n","    pred_fg_mask[layercam > 0.0] = 1\n","\n","    # # Resize if needed\n","    if pred_bg_mask.shape != true_mask.shape:\n","        pred_bg_mask = F.interpolate(pred_bg_mask.unsqueeze(0).unsqueeze(0).float(), size=true_mask.shape[-2:], mode='nearest').squeeze().long()\n","        #pred_bg_mask = F.interpolate(pred_bg_mask.unsqueeze(0).unsqueeze(0).float(), size=true_mask.shape[-2:], mode='nearest').squeeze().long()\n","\n","    iou_bg, acc_bg = compute_iou_and_acc(pred_bg_mask, true_mask)\n","    iou_fg, acc_fg = compute_iou_and_acc(pred_fg_mask, true_mask)\n","\n","    # Visualize overlays\n","    layercam_fg_overlay = overlay_cam_on_image(img, layercam, alpha=0.5, colormap='jet')\n","    layercam_bg_overlay = overlay_cam_on_image(img, layercam_bg, alpha=0.5, colormap='jet')\n","\n","    # Plots\n","    fig, ax = plt.subplots(1, 6, figsize=(18, 6))\n","\n","    ax[0].imshow(layercam_fg_overlay)\n","    ax[0].set_title(\"LayerCAM (M_fg)\")\n","    print(f\"LayerCAM FG: IoU: {iou_fg:.3f} | Acc: {acc_fg:.3f}\")\n","    ax[0].axis('off')\n","\n","    ax[1].imshow(layercam_bg_overlay)\n","    ax[1].set_title(\"LayerCAM (M_bg)\")\n","    print(f\"LayerCAM BG: IoU: {iou_bg:.3f} | Acc: {acc_bg:.3f}\")\n","    ax[1].axis('off')\n","\n","    ax[2].imshow(img.cpu().permute(1, 2, 0).numpy())\n","    ax[2].set_title(\"Original Image\")\n","    ax[2].axis('off')\n","\n","    ax[3].imshow(true_mask.cpu().permute(1, 2, 0).numpy())\n","    ax[3].set_title(\"True Mask\")\n","    ax[3].axis('off')\n","\n","    ax[4].imshow(pred_bg_mask.cpu().numpy())\n","    ax[4].set_title(\"Pred Mask Bg\")\n","    ax[4].axis('off')\n","\n","    ax[5].imshow(pred_fg_mask.cpu().numpy())\n","    ax[5].set_title(\"Pred Mask Fg\")\n","    ax[5].axis('off')\n","\n","\n","    plt.tight_layout()\n","    plt.show()\n","\n","    if p == 5:\n","        break\n","    p += 1"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"96l4z5FcQL0V","executionInfo":{"status":"ok","timestamp":1744572043325,"user_tz":-60,"elapsed":2,"user":{"displayName":"Ajay Mahendrakumaran","userId":"10969685641892920517"}}},"outputs":[],"source":["import os\n","from PIL import Image\n","import torch\n","from torch.utils.data import Dataset\n","import torchvision.transforms.functional as TF\n","import numpy as np\n","\n","class PseudoSegmentationDataset(Dataset):\n","    def __init__(self, img_dir, mask_dir, transform=None):\n","        self.img_dir = img_dir\n","        self.mask_dir = mask_dir\n","        self.image_list = sorted(os.listdir(img_dir))\n","        self.mask_list = sorted(os.listdir(mask_dir))\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.image_list)\n","\n","    def __getitem__(self, idx):\n","        img_path = os.path.join(self.img_dir, self.image_list[idx])\n","        mask_path = os.path.join(self.mask_dir, self.mask_list[idx])\n","\n","        image = Image.open(img_path).convert('RGB')\n","        mask = Image.open(mask_path).convert('L')\n","\n","        if self.transform:\n","            image, mask = self.transform(image, mask)\n","\n","        return image, mask\n","\n","def joint_transform(image, mask):\n","    image = TF.resize(image, (256, 256))\n","    mask = TF.resize(mask, (256, 256), interpolation=Image.NEAREST)\n","\n","    image = TF.to_tensor(image)\n","    image = TF.normalize(image, mean=[0.485, 0.456, 0.406],\n","                                std=[0.229, 0.224, 0.225])\n","    mask = torch.as_tensor(np.array(mask), dtype=torch.long)\n","\n","    return image, mask\n"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"79ANQGKO3nRn","executionInfo":{"status":"ok","timestamp":1744572043327,"user_tz":-60,"elapsed":1,"user":{"displayName":"Ajay Mahendrakumaran","userId":"10969685641892920517"}}},"outputs":[],"source":["import shutil\n","\n","def evaluate_layercam_on_test_set(layercam_gen, test_loader, alpha=1.0, cam_thresh=0.3):\n","    \"\"\"\n","    Evaluates LayerCAM and standard CAM foreground masks on the test set using IoU and pixel accuracy.\n","    \"\"\"\n","    ious_fg, accs_fg = [], []\n","    ious_bg, accs_bg = [], []\n","\n","    for i, (img, (label, true_mask)) in enumerate(test_loader):\n","        img = img[0].cuda()\n","        true_mask = true_mask[0].cuda()\n","\n","        # Binarize true mask: class 1 = foreground\n","        true_mask = (true_mask == 1).long()\n","        label = label[0].item() if isinstance(label[0], torch.Tensor) else label[0]\n","        class_tensor = torch.tensor([label]).to(img.device)\n","\n","        # Generate CAM masks\n","        layercam_output = layercam_gen.generate(img, class_idx=class_tensor, alpha=alpha)\n","\n","        layercam = layercam_output.squeeze(0)\n","\n","        layercam[layercam < cam_thresh] = 0.0\n","\n","        pred_fg_mask = (layercam > 0.0).long()\n","\n","        # Resize if needed\n","        if pred_fg_mask.shape != true_mask.shape:\n","            pred_fg_mask = F.interpolate(pred_fg_mask.unsqueeze(0).unsqueeze(0).float(), size=true_mask.shape[-2:], mode='nearest').squeeze().long()\n","\n","        # Compute metrics\n","        iou_fg, acc_fg = compute_iou_and_acc(pred_fg_mask, true_mask)\n","\n","        ious_fg.append(iou_fg)\n","        accs_fg.append(acc_fg)\n","\n","        if i >= 10:\n","          break # ablations taking too long so only test 10 images\n","\n","\n","    print(\"\\n Evaluation of CAMs on test set:\")\n","    print(f\" - LayerCam FG: Avg IoU: {sum(ious_fg)/len(ious_fg):.4f} | Acc: {sum(accs_fg)/len(accs_fg):.4f}\")\n","\n","\n","    return {\n","        \"layercam_fg_iou\": sum(ious_fg) / len(ious_fg),\n","        \"layercam_fg_acc\": sum(accs_fg) / len(accs_fg)\n","    }\n","\n","\n","import os\n","from torchvision.utils import save_image\n","\n","def generate_pseudo_masks(\n","    loader,\n","    layercam_gen,\n","    cam_thresh=0.3,\n","    alpha=1.0,\n","    keep_largest_masks=True,\n","    run_id=\"default\"):\n","    \"\"\"\n","    Generate pseudo masks using LayerCAM + DenseCRF and save them to disk.\n","    \"\"\"\n","    save_dir = f\"/content/pseudo_masks_{run_id}\"\n","    image_save_dir = f\"/content/images_{run_id}\"\n","    for d in [save_dir, image_save_dir]:\n","      if os.path.exists(d):\n","        shutil.rmtree(d)\n","      os.makedirs(d)\n","\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    img_id = 0\n","\n","    for imgs, (labels, _) in loader:\n","        imgs = imgs.to(device)\n","        labels = labels.to(device)\n","\n","        batch_size = imgs.size(0)\n","\n","        for i in range(batch_size):\n","\n","          # if img_id >= 500:\n","          #   break\n","\n","          img = imgs[i]\n","          label = labels[i].item()\n","\n","          #labels = label.item() if isinstance(label[0], torch.Tensor) else label[0]\n","          class_tensor = torch.tensor([label]).to(device)\n","\n","          cam_bg = layercam_gen.generate(img, class_idx = class_tensor, alpha=alpha)\n","          cam_bg = cam_bg.squeeze(0)\n","          cam_bg[cam_bg < cam_thresh] = 0.0\n","\n","          # Convert image for CRF\n","          image_np = img.detach().cpu().permute(1, 2, 0).numpy()\n","          image_np = (image_np * 255).astype(np.uint8).clip(0, 255)\n","          image_np = np.ascontiguousarray(image_np)\n","\n","          refined_mask = (cam_bg.cpu().numpy() > 0).astype(np.uint8)\n","\n","          #refined_mask = apply_dense_crf(image_np, cam_bg.cpu().numpy(), compat=crf_compat)\n","\n","          if keep_largest_masks:\n","              refined_mask = keep_largest(refined_mask)\n","\n","          # Save pseudo mask\n","          mask_path = os.path.join(save_dir, f\"{img_id}.png\")\n","          save_image(torch.from_numpy(refined_mask).float().unsqueeze(0), mask_path)\n","\n","          # Save image\n","          img_orig = img.cpu().clone()\n","          img_orig = (img_orig - img_orig.min()) / (img_orig.max() - img_orig.min())\n","          save_image(img_orig, os.path.join(image_save_dir, f\"{img_id}.png\"))\n","\n","          img_id += 1\n","\n","    print(f\"Pseudo masks saved to: {save_dir}\")\n","    print(f\"Images saved to: {image_save_dir}\")\n","    return image_save_dir, save_dir\n","\n","\n","def train_segmentation_model(run_id, lr=1e-4, num_epochs=10, batch_size=4):\n","    \"\"\"\n","    Train a DeepLabV3 segmentation model on pseudo masks generated in run `run_id`.\n","    Returns the trained model.\n","    \"\"\"\n","    import torch\n","    import torch.nn as nn\n","    import torchvision\n","    from torch.utils.data import DataLoader\n","    from lovasz_losses import lovasz_softmax  # Import Lovász-Softmax loss\n","    import torch.nn.functional as F  # For softmax\n","\n","    image_dir = f\"/content/images_{run_id}\"\n","    mask_dir = f\"/content/pseudo_masks_{run_id}\"\n","\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","    train_dataset = PseudoSegmentationDataset(\n","        img_dir=image_dir,\n","        mask_dir=mask_dir,\n","        transform=joint_transform\n","    )\n","    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","\n","    num_classes = 2\n","    model = torchvision.models.segmentation.deeplabv3_resnet50(pretrained=True)\n","    model.classifier[4] = nn.Conv2d(256, num_classes, kernel_size=1)\n","    model = model.to(device)\n","\n","    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n","\n","    for epoch in range(num_epochs):\n","        model.train()\n","        total_loss = 0\n","        for images, masks in train_loader:\n","            if images.size(0) == 1:\n","                continue\n","            images, masks = images.to(device), masks.to(device)\n","            masks = torch.clamp(masks, max=1)\n","\n","            outputs = model(images)['out']\n","\n","            probas = F.softmax(outputs, dim=1)\n","\n","            loss = lovasz_softmax(probas, masks, classes='present', per_image=False, ignore=None)\n","\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","\n","            total_loss += loss.item()\n","\n","        final_loss = total_loss\n","        print(f\"[Run {run_id}] Epoch {epoch+1}/{num_epochs}, Loss: {total_loss:.4f}\")\n","\n","    return model, final_loss\n","\n","\n","def evaluate_model(model, test_loader):\n","    \"\"\"\n","    Evaluates a trained segmentation model on a test set.\n","    Returns average IoU and pixel accuracy.\n","    \"\"\"\n","    import torch\n","    import torch.nn.functional as F\n","\n","    device = next(model.parameters()).device\n","    model.eval()\n","\n","    ious, accs = [], []\n","    with torch.no_grad():\n","        for img, (label, true_mask) in test_loader:\n","            img = img[0].to(device)\n","            true_mask = true_mask[0].to(device)\n","            true_mask = (true_mask == 1).long()  # Binarize\n","\n","            output = model(img.unsqueeze(0))['out']\n","            pred_mask = output.argmax(dim=1).squeeze(0)\n","\n","            # Resize if needed\n","            if pred_mask.shape != true_mask.shape:\n","                pred_mask = F.interpolate(pred_mask.unsqueeze(0).unsqueeze(0).float(), size=true_mask.shape[-2:], mode='nearest').squeeze().long()\n","\n","            iou, acc = compute_iou_and_acc(pred_mask, true_mask)\n","            ious.append(iou)\n","            accs.append(acc)\n","\n","    avg_iou = sum(ious) / len(ious)\n","    avg_acc = sum(accs) / len(accs)\n","\n","    print(f\"\\n Model Evaluation on Test Set: IoU = {avg_iou:.4f} | Acc = {avg_acc:.4f}\")\n","    return avg_iou, avg_acc\n"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"ZlBjA53Y3LI3","executionInfo":{"status":"ok","timestamp":1744572043329,"user_tz":-60,"elapsed":1,"user":{"displayName":"Ajay Mahendrakumaran","userId":"10969685641892920517"}}},"outputs":[],"source":["def run_ablation(cam_method, cam_thresh, alpha, lr, keep_largest, run_id):\n","\n","    classifier.load_state_dict(torch.load(save_path))\n","    classifier.eval()\n","\n","    # cam_gen = CAMGenerator(classifier)\n","    layercam_gen = LayerCAMGenerator(classifier, target_layer_names=[\"layer3\", \"layer4\"])\n","\n","    # 1. (Optional) Evaluate LayerCAMs pre-training\n","    #evaluate_layercam_on_test_set(layercam_gen, cam_gen, test_loader, alpha, cam_thresh, cam_thresh_bg)\n","\n","    # 2. Generate pseudo masks\n","    generate_pseudo_masks(loader, layercam_gen, cam_thresh, alpha, keep_largest, run_id)\n","\n","    # 3. Train model\n","    model, final_loss = train_segmentation_model(run_id, lr, num_epochs = 10)\n","\n","    # 4. Evaluate model\n","    iou, acc = evaluate_model(model, test_loader)\n","\n","    return {\"run_id\": run_id, \"iou\": iou, \"acc\": acc, \"final_loss\": final_loss}"]},{"cell_type":"markdown","metadata":{"id":"2v3h2PV5ZkvQ"},"source":["FOR TUSHANN TO RUN:"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"NQsTDdHnZn8W","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1744572043334,"user_tz":-60,"elapsed":4,"user":{"displayName":"Ajay Mahendrakumaran","userId":"10969685641892920517"}},"outputId":"2fe0d208-38c2-451c-b795-63af682e589c"},"outputs":[{"output_type":"stream","name":"stdout","text":["1\n"]}],"source":["import itertools\n","\n","cam_methods = ['LayerCAM']\n","# cam_thresholds = [0.3, 0.5, 0.7]\n","cam_thresholds = [0.3]\n","alphas = [1.0]\n","# lrs = [1e-2, 1e-3, 1e-4, 1e-5]\n","lrs = [1e-4]\n","keep_largest_opts = [True]\n","\n","# Create all combinations\n","all_combinations = list(itertools.product(\n","    cam_methods, cam_thresholds, alphas, lrs, keep_largest_opts\n","))\n","\n","print(len(all_combinations))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IyxgnOf0tiYN","outputId":"8379ae70-52e1-4844-a8ec-2cd35988fb53"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n"," Running abl_000_r0...\n","Pseudo masks saved to: /content/pseudo_masks_abl_000_r0\n","Images saved to: /content/images_abl_000_r0\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DeepLabV3_ResNet50_Weights.COCO_WITH_VOC_LABELS_V1`. You can also use `weights=DeepLabV3_ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","Downloading: \"https://download.pytorch.org/models/deeplabv3_resnet50_coco-cd0a2569.pth\" to /root/.cache/torch/hub/checkpoints/deeplabv3_resnet50_coco-cd0a2569.pth\n","100%|██████████| 161M/161M [00:00<00:00, 205MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["[Run abl_000_r0] Epoch 1/10, Loss: 210.5034\n","[Run abl_000_r0] Epoch 2/10, Loss: 179.7339\n","[Run abl_000_r0] Epoch 3/10, Loss: 165.0054\n","[Run abl_000_r0] Epoch 4/10, Loss: 152.2657\n","[Run abl_000_r0] Epoch 5/10, Loss: 141.3279\n","[Run abl_000_r0] Epoch 6/10, Loss: 131.7467\n","[Run abl_000_r0] Epoch 7/10, Loss: 122.8047\n","[Run abl_000_r0] Epoch 8/10, Loss: 114.0723\n","[Run abl_000_r0] Epoch 9/10, Loss: 107.8236\n","[Run abl_000_r0] Epoch 10/10, Loss: 102.5770\n","\n"," Model Evaluation on Test Set: IoU = 0.4479 | Acc = 0.7921\n","\n"," Running abl_000_r1...\n","Pseudo masks saved to: /content/pseudo_masks_abl_000_r1\n","Images saved to: /content/images_abl_000_r1\n","[Run abl_000_r1] Epoch 1/10, Loss: 211.6483\n","[Run abl_000_r1] Epoch 2/10, Loss: 179.1123\n","[Run abl_000_r1] Epoch 3/10, Loss: 165.8007\n","[Run abl_000_r1] Epoch 4/10, Loss: 152.5699\n","[Run abl_000_r1] Epoch 5/10, Loss: 142.2348\n","[Run abl_000_r1] Epoch 6/10, Loss: 133.3463\n"]}],"source":["import pandas as pd\n","from statistics import mean, stdev\n","\n","results = []\n","num_repeats = 3\n","\n","for combo_id, (method, cam_thresh, alpha, lr, keep_largest_opt) in enumerate(all_combinations):\n","    run_results = []\n","    for repeat in range(num_repeats):\n","        run_id = f\"abl_{combo_id:03d}_r{repeat}\"\n","        print(f\"\\n Running {run_id}...\")\n","\n","\n","        result = run_ablation(\n","                cam_method=method,\n","                cam_thresh=cam_thresh,\n","                alpha=alpha,\n","                lr=lr,\n","                keep_largest=keep_largest,\n","                run_id=run_id\n","            )\n","        result.update({\n","                \"cam_method\": method,\n","                \"cam_thresh\": cam_thresh,\n","                \"alpha\": alpha,\n","                \"learning_rate\": lr,\n","                \"keep_largest\": keep_largest_opt\n","            })\n","        results.append(result)\n","        run_results.append(result)\n","\n","    if run_results:\n","        ious = [r[\"iou\"] for r in run_results]\n","        accs = [r[\"acc\"] for r in run_results]\n","        losses = [r[\"final_loss\"] for r in run_results]\n","\n","        summary = {\n","            \"combo_id\": combo_id,\n","            \"cam_method\": method,\n","            \"cam_thresh\": cam_thresh,\n","            \"alpha\": alpha,\n","            \"learning_rate\": lr,\n","            \"keep_largest\": keep_largest,\n","            \"iou_mean\": mean(ious),\n","            \"iou_std\": stdev(ious) if len(ious) > 1 else 0.0,\n","            \"acc_mean\": mean(accs),\n","            \"acc_std\": stdev(accs) if len(accs) > 1 else 0.0,\n","            \"loss_mean\": mean(losses),\n","            \"loss_std\": stdev(losses) if len(losses) > 1 else 0.0\n","        }\n","        results.append(summary)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QKwHiT60tijo"},"outputs":[],"source":["df = pd.DataFrame(results)\n","df.to_csv(\"ablation_results_False_Lovasz.csv\", index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c0NJst6LaSk4"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"A100","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.9"}},"nbformat":4,"nbformat_minor":0}